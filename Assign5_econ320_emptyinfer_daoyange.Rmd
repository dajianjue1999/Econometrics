---
title: "Assignment Econ 320 Inference Exercises"
author: "Daoyang E"
date: "2019/11/26"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# call packages here
library(wooldridge)
library(tidyverse)
library(stargazer)
library(data.table)
library(car)
library(knitr)
library(lmtest)
```

## Wooldridge C4.11 (Modified) 


Use the data in HTV to answer this question. 

1. Estimate the regression model

$$ educ = \beta_0 + \beta_1 *motheduc + \beta_2 *fatheduc + \beta_3 *abil + \beta_4 *abil^2 + u$$
by OLS take a look at the results in the usual form but report the results using stargazer. 

```{r, results='asis'}
m1 <- lm(educ ~ motheduc + fatheduc + abil + I(abil^2), data = htv)
stargazer(list(m1), type="html", align=TRUE,
          covariate.labels = c("motheduc", "fatheduc", "abil", "I(abil2)"),
          dep.var.caption  = "Dependent Variable:",
          dep.var.labels   = "educ")
```


2.a Test the null hypothesis that educ is linearly related to abil against the alternative that the relationship is quadratic. Show the tstat and the pvalue associated with that test. Do you reject the null hypothesis? what does that mean? EXPLAIN

```{r}
print("T-test for linear vs quadratic relation on ability")
regtable <- summary(m1)$coefficients
abilest <- regtable[5,1]
abilse <- regtable[5,2]
tstat <- round((abilest - 0) / abilse, 4)
print(paste(tstat))
pvalue <- round(regtable[5,4], 4)
print(paste("pvalue = ", pvalue))

```

I would reject my null hypothesis, since it has statistical significance, this means that education, for other factors controlled, is quadratically related to ability

2.b Test the null hypothesis that the effect of mothereduc is bigger 0.3. 

```{r}
print("T-test for mother educ > 0.3")
mothest <- regtable[2,1]
mothse <- regtable[2,2]
mtstat <- round((mothest - 0.3) / mothse, 4)
print(paste(mtstat))
mpvalue <- 1-regtable[2,4]
print(paste("pvalue = ", mpvalue))
```


<br>

3.  Using the equation in part (2), test $H_0: \beta_1=\beta_2$ against a two-sided alternative. What is the p-value of the test? 

Remember this requires for creating a new regression with a $\theta_1=\beta_1-\beta_2$ as shown in your book and then test for $H_0: \theta_1=0$

 Let's change the regression to create $\theta_1=\beta_1-\beta_2$ 

Add and subrstact $\beta_2 motheduc$ and create a variable $parentedu=motheduc+fatheduc$ see below: 

$$ educ = \beta_0 + \beta_1 motheduc - \beta_2 motheduc + \beta_2 motheduc+ \beta_2 fatheduc + \beta_3 abil + \beta_4 abil^2 + u$$

$$ educ = \beta_0 + (\beta_1 - \beta_2)   motheduc + \beta_2  (motheduc+fatheduc) + \beta_3 abil + \beta_4 abil^2 + u$$
$$ educ = \beta_0 + \theta_1   motheduc + \beta_2  (parentedu) + \beta_3 abil + \beta_4 abil^2 + u$$

By testing the null hypothesis that $H_0:\theta_1=0$ with $alpha=0.05$ we are testing $H_0: \beta_1=\beta_2$ So we just run the regression that has $\theta_1$ as a regressor and look at the t-test for $\theta_1$

```{r, results='hold'}
#critival Values for alpha=5% and 1% for 1225 degrees of freedom 
print("critical values for alpha 5% and 1% 2 tails")
(alpha_2 <- c(-1.959964, -2.575829))

htv <- htv %>% mutate(parenteduc = motheduc +  fatheduc)

m2 <- lm(educ ~ motheduc + parenteduc + abil + I(abil^2), data = htv)
stargazer(m2, type="text") 
regtable2 <- round(summary(m2)$coefficients, 3)
# create parenteduc

# regression with theta1

```

***
**Use in-line code and your interpretation for this paragraph** the value of $\theta_1$ is equal to `r regtable2[2,1]` with a t-stat of `r regtable[2,3]` and a p-value of `r regtable[2,4]` this means that we do not reject the null hypothesis that  $H_0:\theta_1=0$ which means that $\beta_1$ is equal to $\beta_2$ therefore the level of education of mother's and father's does not differ on education magnitute. 

***
<br>

4. 	Add the two college tuition variables to the regression from part (2) and determine whether they are jointly statistically significant. 
First do the F-test step-by-step

```{r Ftest}
# CV for alpha=1% using the F distribution with 1223 degrees of fredom d.f. :
print(paste("Critical value at 1% with 2 and 1223 df=",qf(1-0.01, 2,1223)))  

## F test step by step
# Unrestricted OLS regression:  
res.ur<- lm(educ ~ motheduc + fatheduc + abil + I(abil^2) + tuit17 + tuit18, data = htv)
# Restricted OLS regression:
res.r <- lm(educ ~ motheduc + fatheduc + abil + I(abil^2), data = htv)
# R2:
r2.ur <- summary(res.ur)$r.squared # R squared unrestricted
r2.r <- summary(res.r)$r.squared # R squared restricted 
print(paste("$R^2$ unrestricted=", r2.ur))
print(paste("$R^2$ restricted=", r2.r))
# F statistic:
F <- (r2.ur-r2.r) / (1-r2.ur) * 1223/2
print(paste("F-stat=", F))
# p value = 1-cdf of the appropriate F distribution:
print(paste("p-value=", round(pf(F, 2,1223),9)))
```
<br>
***
Then use any of the other methods
 <br>
```{r}
# F test 
myH0 <- c("tuit17","tuit18")
linearHypothesis(res.ur, myH0)
# anova(res, res.ur)
anova(res.r, res.ur)
```


<br> 
This shows that in this case we **do not reject the null hypothesis**, that the coefficients are jointly zero. 

***
5.  Use function `confint()` to find the confidence intervals of all the parameters in the unsrestricted model from (4) What do you conclude? EXPLAIN this results in the light of the significance of your coeficients

```{r}
confint(res.ur, level = 0.95)
```

This table concludes the 95% confidence limits for each parameter, I would conclude that tuit17 and tuit18 does not seem to have strong influence on education, and this conclusion is in correspondence with our null hypothesis

6. Using the Breush-Pagan test, test for heteroskedasticity in your  model  
$$ educ = \beta_0 + \beta_1 *motheduc + \beta_2 *fatheduc + \beta_3 *abil + \beta_4 *abil^2 + u$$ 
then estimate the model with robust standard errors (correcting for the heteroskedasticy problem), and the present both ( OLS and robust) the results in a table using `screenreg()`. 

Do the significance of your results change after the correction? What about the standard errors?

```{r}
bptest(m1)
regrobw<-coeftest(m1, vcov = hccm)
stargazer(list(m1, regrobw), type="text",star.cutoffs = c(0.1, 0.05, 0.01))
```


***
<style>
div.gray { background-color:#dbdbdb; border-radius: 5px; padding: 20px;}
</style>
<div class = "gray">

**Packages used in this document**

library(wooldridge)
library(tidyverse)
library(stargazer)
library(data.table)
library(car)
library(knitr)
library(lmtest)

</div>
<br>
<br>







